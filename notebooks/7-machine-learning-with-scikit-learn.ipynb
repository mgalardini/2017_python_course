{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning with scikit-learn\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[scikit-learn](http://scikit-learn.org/stable/) is the most used python library for machine learning (but you might want to look into [TensorFlow](https://www.tensorflow.org/) and [Theano](http://deeplearning.net/software/theano/) if you like deep-learning). It includes both supervised and unsupervised learning, clustering, feature reduction (PCA) and several pre-processing and performance metrics.\n",
    "\n",
    "Despite the great number of algorithms implemented in the library, the structure is similar for all of them, making it easier to use them and plug them together. Advanced usage include pipelines and advanced cross-validations.\n",
    "\n",
    "Key components of scikit-learn are the following:\n",
    "\n",
    "* Estimators: an object that can estimate some parameters based on a dataset (e.g an `Imputer`, which imputes missing values)\n",
    "* Transformers: a subset of estimators that can also act on the original dataset to alter its values, using the parameters learned in the estimation step (e.g. `MinMaxScaler`, which standardises the given dataset)\n",
    "* Predictors: object capable of making predictions (e.g. `LinearRegression`, which as you might have guessed makes a linear regression)\n",
    "\n",
    "All of this components (actually classes) have consistent characteristics:\n",
    "\n",
    "* a `fit` method to train the model or learn some parameters\n",
    "* a `transform` (or `fit_transform`) method to transform the data\n",
    "* a `predict` method to carry out the predictions\n",
    "* a `score` method to asses the goodness of the predictions\n",
    "* learned parameters and hyperparameters are exposed as public `attributes`, by convention followed by an underscore (e.g. `Ridge`'s `alpha_` parameter)\n",
    "\n",
    "This consistency allows great flexibility: different machine-learning methods and algorithms can be swapped easily, and pipelines can be easily built, since each class can feed into each other's result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm should I choose?\n",
    "----------------------\n",
    "\n",
    "![algo-cheat-sheet](http://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: predictor of breast cancer malignity**\n",
    "\n",
    "Datasets in `scikit.learn` follow the scheme `(n_samples, n_features)`, using `numpy.ndarrays`. If you want greater flexibility, it is advisable to use pandas dataframes.\n",
    "\n",
    "This dataset is available inside `scikit-learn` as a test dataset. Playing with it will provide a good introduction to the main concepts of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting imports\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(d['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d['data'],\n",
    "                  columns=d['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.Series(d['target'],\n",
    "                   name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the samples seem to have all the features, so we will not need to either impute them or drop them.\n",
    "\n",
    "A quick look at the features\n",
    "--------------\n",
    "\n",
    "Despite not being strictly implemented in `scikit-learn`, having a broad overview of our input dataset is a mandatory step in any machine learning project. We can use either `panda`'s plotting capabilities or `matplotlib`/`seaborn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are not on the same scale, despite all following similar distributions. This could be a problem for some algorithms. We will therefore need to perform *feature scaling*, of which there are two flavours:\n",
    "\n",
    "* Normalization: scaling a feture between a `min` and a `max` value (i.e. through the `MinMaxScaler` *transformer*)\n",
    "* Standardization: bring the feature of interest to a desired mean and std-dev\n",
    "    * much less affected by outliers\n",
    "    \n",
    "Another routine operation is to verify the relationship between features, so that highly correlated ones are either combined or selected. Again, we can have a quick sense using either `seaborn` or `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df[df.columns[:5]], size=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df[df.columns[:5]], figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage `seaborn`'s `clustermap` method to visualize the correlation matrix of all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mclust = sns.clustermap(df.corr(),\n",
    "                        figsize=(10, 10),\n",
    "                        cmap='RdBu')\n",
    "mclust.ax_heatmap.set_yticklabels(mclust.ax_heatmap.get_yticklabels(),\n",
    "                                  rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that it might be a good idea to either drop some features or create new ones (*features engineering*) to improve our predictor. For instance we could highlight those features that have at least a correlation above 0.7 and randomly pick a representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_corr = df.corr()\n",
    "feat_corr.stack().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_corr = df.corr()\n",
    "# ignore the diagonal, obviously\n",
    "np.fill_diagonal(feat_corr.values, np.nan)\n",
    "feat_corr = feat_corr.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_corr[feat_corr > 0.7].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can you think a smarter way to perform this operation?\n",
    "high_corr = feat_corr[feat_corr > 0.7]\n",
    "discarded = set()\n",
    "saved = set()\n",
    "for feat1 in {x[0] for x in high_corr.index}:\n",
    "    if feat1 in discarded:\n",
    "        continue\n",
    "    saved.add(feat1)\n",
    "    for feat2 in high_corr.loc[feat1].index:\n",
    "        discarded.add(feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df.loc[:, sorted(set(df.columns) - discarded)], size=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[:, sorted(set(df.columns) - discarded)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a training and test set\n",
    "--------------\n",
    "\n",
    "To avoid the risk of *confirmation bias*, and most importanlty *overfitting*, it is also a good practice to split early our dataset into two:\n",
    "\n",
    "* *training set*: used to train our predictor\n",
    "* *test set*: used to verify the appropriatness of our trained predictor\n",
    "\n",
    "You should avoid *at all costs* the temptation to use the test set until the very end of the project, and only to choose which model gives the best trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_test = model_selection.train_test_split(df,\n",
    "                                                     test_size=0.2)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_train, target_test = target.iloc[df_train.index], target.iloc[df_test.index]\n",
    "target_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_train[target_train == 0].shape[0] / target_train[target_train == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_test[target_test == 0].shape[0] / target_test[target_test == 1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We might have a problem here: the proportion of benign and malign tumors is uneven between the train and test sets! This might impose a bias in our predictor, which will reflect in a poor performance in our final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = model_selection.StratifiedShuffleSplit(n_splits=1,\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StratifiedShuffleSplit` class is a more advanced way to generate a test set, such that the proportion of each target class are roughly the same with the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are defining a `random_state` so that the random number generator seed is the same each time we run this notebook. This ensures that we will always get the same train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv.split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx = next(cv.split(df, target))\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, target_train = df.iloc[train_idx], target[train_idx]\n",
    "df_test, target_test = df.iloc[test_idx], target[test_idx]\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_train[target_train == 0].shape[0] / target_train[target_train == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_test[target_test == 0].shape[0] / target_test[target_test == 1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now gonna focus exclusively on the train set. If we end up applying any transformation to the data, we'll make sure to apply the same transformation to the test set, using the parameters learned on the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning\n",
    "-------------\n",
    "\n",
    "After we have reduced the number of features we are going to standardize the remaining ones. Importantly, we are going to use the same `transformer` later for the test set, in oder to make use of exactly the same transormation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "std_scaler = preprocessing.StandardScaler()\n",
    "std_scaler.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, whenever we apply a `fit` method, the returned value is the same `instance` of the used `transformer`. To make things easier, we can use the `fit_transform` method, which fits the scaler using the provided data and then applies the transormation to the same input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "std_scaler.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's bring it back to a dataframe\n",
    "df_scaled = pd.DataFrame(std_scaler.fit_transform(df_train),\n",
    "                         columns=df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_scaled.hist(bins=50, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the appropriate machine learning algorithm\n",
    "------------------------\n",
    "\n",
    "Based on the cheatsheet above, we are facing a *classification* problem, meaning that we want a predictor to be able to input our features and output a classification label. This particular dataset is easy in this sense, because we are going to work with a binary classifier. The above cheat-sheet seems to indicate that the best classifier given the limited amount of data points might be a *support vector machine with a linear kernel* (`LinearSVC`).\n",
    "\n",
    "![classifiers](http://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
    "\n",
    "The above scheme highlights the strength of the different classifiers available, which also is a powerful indication that it's always a good idea to visualize your data. Different structures in the data will dictate different optimal strategies and therefore different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC()\n",
    "clf = clf.fit(df_scaled, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are passing both the features and the labels to the `fit` method. The `predict` method will apply the learned *decision boundaries* to the input features to produce the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_train = pd.Series(clf.predict(df_scaled),\n",
    "                          index=target_train.index,\n",
    "                          name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation\n",
    "---------\n",
    "\n",
    "How are we going to decice whether our classifier is good enough to be used in the clinic? Scikit-learn offers several metrics that we can use, each better suited for each predictive task. Let's first try a rough comparison of the *true* and *predicted* labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = target_train.to_frame().join(predict_train.to_frame())\n",
    "combined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined[combined['target'] == combined['prediction']].shape[0] / combined.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first some more evaluation metrics, such as a [*ROC*](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and a [*precision-recall*](https://en.wikipedia.org/wiki/Precision_and_recall) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(combined['target'],\n",
    "                                         combined['prediction'])\n",
    "plt.plot(fpr, tpr,\n",
    "         'r-')\n",
    "plt.plot([0, 1],\n",
    "         [0, 1],\n",
    "         '--',\n",
    "         color='grey')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "prec, rec, thresholds = metrics.precision_recall_curve(combined['target'],\n",
    "                                                        combined['prediction'])\n",
    "plt.plot(rec, prec,\n",
    "         'r-')\n",
    "\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "metrics.roc_auc_score(combined['target'],\n",
    "                      combined['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% accuracy and 0.98 ROC AUC! Well done...\n",
    "\n",
    "...but what if we are actually overfitting the input data? This might be especially true when many features are used, leading to the [curse of dimensionality](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/). And we are also using the dafault parameters of our classifier, which might also play in inducing overfitting. We might therefore need to use *cross validation* on the train set, further splitting it into a *train* and a *validation* set. Luckily `scikit-learn` offers an utility function to to this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_selection.cross_val_score(clf, df_scaled, target_train,\n",
    "                                cv=10,\n",
    "                                scoring=metrics.make_scorer(metrics.roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that we are not actually overfitting our data, at least judging from this simple analysis.\n",
    "\n",
    "But what about picking the best hyperparameters? Wec ould keep it simple and test a range of values for the `C`  and `loss` parameters, using another useful `class` of `scikit learn`: `GridSearchCV`, which will explore all combinations of the parameters, using a scoring function to asses their impact on the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': np.linspace(0.01, 10),\n",
    "              'loss': ['hinge', 'squared_hinge']}\n",
    "\n",
    "clf = svm.LinearSVC()\n",
    "grid_search = model_selection.GridSearchCV(clf,\n",
    "                                           param_grid=param_grid,\n",
    "                                           cv=10,\n",
    "                                           scoring=metrics.make_scorer(metrics.roc_auc_score))\n",
    "grid_search.fit(df_scaled, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "\n",
    "for mean_score, params in zip(grid_search.cv_results_[\"mean_test_score\"],\n",
    "                              grid_search.cv_results_[\"params\"]):\n",
    "    if params['loss'] == 'hinge':\n",
    "        plt.plot(params['C'],\n",
    "                 mean_score,\n",
    "                 'bo')\n",
    "    else:\n",
    "        plt.plot(params['C'],\n",
    "                 mean_score,\n",
    "                 'ro')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('ROC AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the combination of `loss` = `hinge` and `C` = 2 produces the best results. There are other hyperparameters that we could tune and better metrics we could use, but we'll stop here and test our trained model on the test dataset. \n",
    "\n",
    "Pipelines\n",
    "----------\n",
    "\n",
    "We'll do it in one go using another very useful feature of `scikit-learn`: pipelines. Pipelines allow the user to chain several transformers (every piece of the pipeline but the last have to implement the `fit_transform` method) and a final estimator. This way we can keep the process both modular and concise. We'll also be able to pass to the final estimator (or any extra transformer we might have) the learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breast_pipeline = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()),\n",
    "                                     ('classifier', svm.LinearSVC(C=2., loss='hinge'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breast_pipeline = breast_pipeline.fit(df_train, target_train)\n",
    "breast_pipeline.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_test = pd.Series(breast_pipeline.predict(df_test),\n",
    "                         index=target_test.index,\n",
    "                         name='prediction')\n",
    "combined = target_test.to_frame().join(predict_test.to_frame())\n",
    "combined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(combined['target'],\n",
    "                                         combined['prediction'])\n",
    "plt.plot(fpr, tpr,\n",
    "         'r-')\n",
    "plt.plot([0, 1],\n",
    "         [0, 1],\n",
    "         '--',\n",
    "         color='grey')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "prec, rec, thresholds = metrics.precision_recall_curve(combined['target'],\n",
    "                                                        combined['prediction'])\n",
    "plt.plot(rec, prec,\n",
    "         'r-')\n",
    "\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "metrics.roc_auc_score(combined['target'],\n",
    "                      combined['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model doesn't work just as good as for the training set, which is a good sign that we are have not produced an overfitted model. Our predictions are still very good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset example: iris\n",
    "---------------\n",
    "\n",
    "In the following dataset we are still trying to classify our data, but this time we have three classes. For this reason we might want to try some clusterization methods.\n",
    "\n",
    "![clusterization](http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# rows are observations, columns are features\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# true label for each observation\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# label names\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# the simplest preprocessing is to standardize the data\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "iris.data = std_scaler.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# once the model has been fitted, we can add a new observation and can try to predict to which cluster they belong to\n",
    "kmeans.predict([[5.8,  2.7,  4.0,  1.25],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to plot the dataset (using only two features for semplicity), together with the true labels (glyphs) and the clusters (colors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(121)\n",
    "for label, glyph in zip(set(iris.target), ('o', 'D', '^')):\n",
    "    for cluster, color in zip(set(kmeans.labels_), ('b', 'r', 'g')):\n",
    "        plt.plot(iris.data[(iris.target == label) & (kmeans.labels_ == cluster)][:, 0],\n",
    "                 iris.data[(iris.target == label) & (kmeans.labels_ == cluster)][:, 1],\n",
    "                 marker=glyph,\n",
    "                 linestyle='',\n",
    "                 color=color,\n",
    "                 label='{0} - {1}'.format(iris.target_names[label],\n",
    "                                          cluster))\n",
    "plt.xlabel('feature 0')\n",
    "plt.ylabel('feature 1')\n",
    "\n",
    "plt.subplot(122)\n",
    "for label, glyph in zip(set(iris.target), ('o', 'D', '^')):\n",
    "    for cluster, color in zip(set(kmeans.labels_), ('b', 'r', 'g')):\n",
    "        plt.plot(iris.data[(iris.target == label) & (kmeans.labels_ == cluster)][:, 2],\n",
    "                 iris.data[(iris.target == label) & (kmeans.labels_ == cluster)][:, 3],\n",
    "                 marker=glyph,\n",
    "                 linestyle='',\n",
    "                 color=color,\n",
    "                 label='{0} - {1}'.format(iris.target_names[label],\n",
    "                                          cluster))\n",
    "plt.xlabel('feature 2')\n",
    "plt.ylabel('feature 3')\n",
    "plt.legend(loc=(1, 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering is separating the three categories with a decent discriminative power; we can use some metrics implemented in scikit-learn to be more precise. We are going to use an homogeinity score to measure how \"pure\" each cluster is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "metrics.homogeneity_score(iris.target, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear model classifier example**\n",
    "\n",
    "In a real world scenario we would need to divide our dataset into a training and test set; for that purpose the `sklearn.cross_validation` module should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier(alpha=1.0)\n",
    "ridge.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = ridge.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "metrics.f1_score(iris.target, predictions, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `alpha` parameter we are probably not overfitting the model, as we are not correctly predicting even the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(iris.data[iris.target == predictions][:, 0],\n",
    "         iris.data[iris.target == predictions][:, 1],\n",
    "         'k.',\n",
    "         label='correct predictions')\n",
    "plt.plot(iris.data[iris.target != predictions][:, 0],\n",
    "         iris.data[iris.target != predictions][:, 1],\n",
    "         'ro',\n",
    "         label='incorrect predictions')\n",
    "plt.xlabel('feature 0')\n",
    "plt.ylabel('feature 1')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(iris.data[iris.target == predictions][:, 2],\n",
    "         iris.data[iris.target == predictions][:, 3],\n",
    "         'k.',\n",
    "         label='correct predictions')\n",
    "plt.plot(iris.data[iris.target != predictions][:, 2],\n",
    "         iris.data[iris.target != predictions][:, 3],\n",
    "         'ro',\n",
    "         label='incorrect predictions')\n",
    "plt.xlabel('feature 2')\n",
    "plt.ylabel('feature 3')\n",
    "plt.legend(loc=(1, 0));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
